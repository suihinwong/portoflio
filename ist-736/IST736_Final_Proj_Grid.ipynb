{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025646ef",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "054e89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\Morga\\programsMG\\TextMining\\Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91a36c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the row \n",
    "df.drop([314], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2aeda",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f87e8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the label\n",
    "y=df['sentiment'].values\n",
    "M=df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4760697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "X1=[]\n",
    "for item in M:\n",
    "    X1.append(re.sub(r'([a-zA-Z0-9_<>-])\\1+', r'\\1\\1', item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df27b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempX=[]\n",
    "for item in X1:\n",
    "    iteml=item.lower()\n",
    "    item1=iteml.replace('2day ','today ')\n",
    "    item2=item1.replace('2moro ','tomorrow ')\n",
    "    item3=item2.replace('2morrow ','tomorrow ')\n",
    "    item4=item3.replace('2night ','tonight ')\n",
    "    item5=item4.replace('2nite ','tonight ')\n",
    "    item6=item5.replace('b-day ','birthday ')\n",
    "    item7=item6.replace('b4 ','before ')\n",
    "    item8=item7.replace('bb ','be back ')\n",
    "    item9=item8.replace('bbl ','be back later ')\n",
    "    item10=item9.replace('bc ','because ')\n",
    "    item11=item10.replace('bday ','birthday ')\n",
    "    item12=item11.replace('belive ','believe ')\n",
    "    item13=item12.replace('bf ','boyfriend ')\n",
    "    item14=item13.replace('bff ','best freind forever ')\n",
    "    item15=item14.replace('brb ','be right back ')\n",
    "    item16=item15.replace('bros ','bro ')\n",
    "    item17=item16.replace('bs ','bullshit ')\n",
    "    item18=item17.replace('btw ','by the way ')\n",
    "    item19=item18.replace('dat ','that ')\n",
    "    item20=item19.replace('doc ','doctor ')\n",
    "    item21=item20.replace('docs ','doctor ')\n",
    "    item22=item21.replace('hott ','hot ')\n",
    "    item23=item22.replace('fb ','facebook ')\n",
    "    item24=item23.replace('jk ','just kidding ')\n",
    "    item25=item24.replace('jst ','just ')\n",
    "    item26=item25.replace(' ng ',' nice game ')\n",
    "    item27=item26.replace('nt ','nice try ')\n",
    "    item28=item27.replace('ok ','okay ')\n",
    "    item29=item28.replace('okayy ','okay ')\n",
    "    item30=item29.replace('omgg ','omg ')\n",
    "    item31=item30.replace('ppl ','people ')\n",
    "    item32=item31.replace('tonite ','tonight ')\n",
    "    item33=item32.replace(' u ',' you ')\n",
    "    item34=item33.replace('u2 ','you too ')\n",
    "    item35=item34.replace('ugg ','ugh ')\n",
    "    item36=item35.replace('uggh ','ugh ')\n",
    "    item37=item36.replace('uh ','ugh ')\n",
    "    item38=item37.replace('uhh ','ugh ')\n",
    "    item39=item38.replace('umm ','um ')\n",
    "    item40=item39.replace('ur ','your ')\n",
    "    item41=item40.replace('waah ','waa ')\n",
    "    item42=item41.replace('wah ','waa ')\n",
    "    item43=item42.replace('waay ','waa ')\n",
    "    item44=item43.replace('xoxo ','xo ')\n",
    "    item45=item44.replace('xx ','xo ')\n",
    "    item46=item45.replace(' y ',' why ')\n",
    "    item47=item46.replace('whyy ','why ')\n",
    "    item48=item47.replace('yaay ','ya ')\n",
    "    item49=item48.replace('yah ','ya ')\n",
    "    item50=item49.replace('juss ','just ')\n",
    "    item51=item50.replace('whassqoodd ','whats good') \n",
    "    item52=item51.replace('soo ','so ')\n",
    "    tempX.append(item52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "259068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TweetTokenizer() method from nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62f3bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for the tweet tokenizer from NLTK\n",
    "def tok(text):\n",
    "    tt = TweetTokenizer()\n",
    "    return tt.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69c4a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the stop words\n",
    "stopwords =['!','#','$','%','&',\"'\",'*','(',')','+', ',','-','.','/',':',';','<','=','>'\n",
    "              ,'?','@','[','\\\\',']','^','_','`','{','|','}','~','Â¿','#bgt','#fail','#fb'\n",
    "              ,'#ff','#followfriday','#hhrs','#judday','#sanctuarysunday','#starwarsday'\n",
    "              ,'#twpp','>','_','_127','__','_b','_benson','_c','_carter','_d','_guy','_henrie'\n",
    "              ,'_j','_m','_marie','_skies','_x','_xo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf6580fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function for the pipeline to remove stop words\n",
    "def remove_stop(tokens):\n",
    "    return [t for t in tokens if t not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63f223bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to use each function from the pipeline on the texts\n",
    "def prepare(text, pipeline):\n",
    "    tokens = text\n",
    "    for transform in pipeline:\n",
    "        tokens = transform(tokens)\n",
    "    return tokens     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38f1fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the functions for the pipeline\n",
    "pipeline = [tok,remove_stop,]\n",
    "# using the pipelined list of functions.\n",
    "Xtemp=[]\n",
    "for tweet in tempX:\n",
    "    Xtemp.append(prepare(tweet,pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b0d1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new list\n",
    "X=[]\n",
    "#joining the lists of lists\n",
    "#so a list of stirngs remains\n",
    "for tweet in Xtemp:\n",
    "    X.append(' '.join(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa0248b",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "516bcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "667b2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "unigram_tfidf_vectorizer_Tweet_toke_no_stop_stem = TfidfVectorizer(encoding='latin-1'\n",
    "                                                                   , use_idf=True\n",
    "                                                                   , min_df=5\n",
    "                                                                   ,tokenizer=tok\n",
    "                                                                   ,analyzer=stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65fd3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = unigram_tfidf_vectorizer_Tweet_toke_no_stop_stem.fit_transform(X)\n",
    "\n",
    "vecsdf=pd.DataFrame(vecs.toarray(),\n",
    "            columns=unigram_tfidf_vectorizer_Tweet_toke_no_stop_stem.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bad7e4",
   "metadata": {},
   "source": [
    "## Adding Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f6cb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for review in X:\n",
    "    words.append(tok(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35896336",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count =[]\n",
    "for review in words:\n",
    "    count =0\n",
    "    for word in review:\n",
    "        count=count+1\n",
    "    word_count.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2898585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word_array = np.asarray(word_count)\n",
    "word_norm = (word_array - word_array.min())/ (word_array.max() - word_array.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78731882",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecsdf['word_norm']=word_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90bd9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def has_negation(post):\n",
    "    pattern_neg_1 = re.compile(r'\\b(not|no|never)\\b')\n",
    "    pattern_neg_2 = re.compile(r'\\b([a-z]+less)\\b')\n",
    "    if pattern_neg_1.search(post.lower()) or pattern_neg_2.search(post.lower()):\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8933288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_count=[]\n",
    "for item in X:\n",
    "    neg_count.append(has_negation(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abd09380",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_array = np.asarray(neg_count)\n",
    "neg_norm = (neg_array - neg_array.min())/ (neg_array.max() - neg_array.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e86c16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecsdf['neg_norm']=neg_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf47f9",
   "metadata": {},
   "source": [
    "## Preforming Gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a01eef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#initialize the model\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "413fc630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV 1/3] END ..............C=0.1, kernel=linear;, score=0.630 total time=18.9min\n",
      "[CV 2/3] END ..............C=0.1, kernel=linear;, score=0.629 total time=18.0min\n",
      "[CV 3/3] END ..............C=0.1, kernel=linear;, score=0.634 total time=18.6min\n",
      "[CV 1/3] END ................C=1, kernel=linear;, score=0.708 total time=14.4min\n",
      "[CV 2/3] END ................C=1, kernel=linear;, score=0.712 total time=14.3min\n",
      "[CV 3/3] END ................C=1, kernel=linear;, score=0.713 total time=14.4min\n",
      "[CV 1/3] END ...............C=10, kernel=linear;, score=0.678 total time=16.4min\n",
      "[CV 2/3] END ...............C=10, kernel=linear;, score=0.680 total time=16.5min\n",
      "[CV 3/3] END ...............C=10, kernel=linear;, score=0.674 total time=16.5min\n",
      "[CV 1/3] END ..............C=100, kernel=linear;, score=0.646 total time=36.0min\n",
      "[CV 2/3] END ..............C=100, kernel=linear;, score=0.651 total time=36.3min\n",
      "[CV 3/3] END ..............C=100, kernel=linear;, score=0.646 total time=35.6min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 797.43565663,  614.29128806,  776.90750162, 1965.3668735 ]),\n",
       " 'std_fit_time': array([20.28127319,  2.2663014 ,  4.06782015, 17.79826103]),\n",
       " 'mean_score_time': array([312.66564234, 249.85184956, 211.75202394, 193.0925192 ]),\n",
       " 'std_score_time': array([8.80795456, 0.66921283, 0.15327728, 0.23836649]),\n",
       " 'param_C': masked_array(data=[0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_kernel': masked_array(data=['linear', 'linear', 'linear', 'linear'],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1, 'kernel': 'linear'},\n",
       "  {'C': 1, 'kernel': 'linear'},\n",
       "  {'C': 10, 'kernel': 'linear'},\n",
       "  {'C': 100, 'kernel': 'linear'}],\n",
       " 'split0_test_score': array([0.63045852, 0.70840611, 0.67772926, 0.64606987]),\n",
       " 'split1_test_score': array([0.62925764, 0.71200873, 0.68002183, 0.65131004]),\n",
       " 'split2_test_score': array([0.63438865, 0.71331878, 0.67358079, 0.64552402]),\n",
       " 'mean_test_score': array([0.63136827, 0.71124454, 0.67711063, 0.64763464]),\n",
       " 'std_test_score': array([0.00219128, 0.00207711, 0.00266568, 0.00260844]),\n",
       " 'rank_test_score': array([4, 1, 2, 3])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#param_grid = {'C': [0.1, 1, 10, 100],'kernel': ['linear']}  \n",
    "#grid_svm_ngram = GridSearchCV(svc, param_grid, refit = True, verbose = 3, cv=3) \n",
    "#grid_svm_ngram.fit(vecsdf, y)\n",
    "#grid_svm_ngram.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "851ec88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_ngram_df=pd.DataFrame(grid_svm_ngram.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de769ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>797.435657</td>\n",
       "      <td>20.281273</td>\n",
       "      <td>312.665642</td>\n",
       "      <td>8.807955</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.1, 'kernel': 'linear'}</td>\n",
       "      <td>0.630459</td>\n",
       "      <td>0.629258</td>\n",
       "      <td>0.634389</td>\n",
       "      <td>0.631368</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>614.291288</td>\n",
       "      <td>2.266301</td>\n",
       "      <td>249.851850</td>\n",
       "      <td>0.669213</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1, 'kernel': 'linear'}</td>\n",
       "      <td>0.708406</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.713319</td>\n",
       "      <td>0.711245</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>776.907502</td>\n",
       "      <td>4.067820</td>\n",
       "      <td>211.752024</td>\n",
       "      <td>0.153277</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 10, 'kernel': 'linear'}</td>\n",
       "      <td>0.677729</td>\n",
       "      <td>0.680022</td>\n",
       "      <td>0.673581</td>\n",
       "      <td>0.677111</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1965.366874</td>\n",
       "      <td>17.798261</td>\n",
       "      <td>193.092519</td>\n",
       "      <td>0.238366</td>\n",
       "      <td>100</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 100, 'kernel': 'linear'}</td>\n",
       "      <td>0.646070</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.645524</td>\n",
       "      <td>0.647635</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0     797.435657     20.281273       312.665642        8.807955     0.1   \n",
       "1     614.291288      2.266301       249.851850        0.669213       1   \n",
       "2     776.907502      4.067820       211.752024        0.153277      10   \n",
       "3    1965.366874     17.798261       193.092519        0.238366     100   \n",
       "\n",
       "  param_kernel                          params  split0_test_score  \\\n",
       "0       linear  {'C': 0.1, 'kernel': 'linear'}           0.630459   \n",
       "1       linear    {'C': 1, 'kernel': 'linear'}           0.708406   \n",
       "2       linear   {'C': 10, 'kernel': 'linear'}           0.677729   \n",
       "3       linear  {'C': 100, 'kernel': 'linear'}           0.646070   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.629258           0.634389         0.631368        0.002191   \n",
       "1           0.712009           0.713319         0.711245        0.002077   \n",
       "2           0.680022           0.673581         0.677111        0.002666   \n",
       "3           0.651310           0.645524         0.647635        0.002608   \n",
       "\n",
       "   rank_test_score  \n",
       "0                4  \n",
       "1                1  \n",
       "2                2  \n",
       "3                3  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svm_ngram_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
